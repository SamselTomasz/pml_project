---
title: "Practical Machine Learning Project"
author: "Tomasz Samsel"
date: "11/22/2015"
output: html_document
---
## Project writeup.

#### Background.
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://groupware.les.inf.puc-rio.br/har). (see the section on the Weight Lifting Exercise Dataset).

#### Data.
The training data for this project are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The test data are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The data for this project come from this [source](http://groupware.les.inf.puc-rio.br/har). If you use this document for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of projects.

#### The goal of project.
The goal of this project is to predict the manner in which participants did the exercise. This is the "classe" variable in the training set. Any of the other variables can be used to predict with. Finally prediction model to predict 20 different test cases will be created. For assigment purposes the code will generate 20 files with predicted manner of witch the excersize was perform. Test cases data is a test dataset. For link see above.

## Loading and cleaning data.

#### Loading data.
The code below will check if data has been downloaded already. If not, it will download it, and load it to memory.
Training dataset should contain just below 20000 cases of 160 variables. Test dataset should be exactly 20 cases long.
```{r, cache=TRUE}
## download files if they do not exist 
if (!file.exists("pml_training.csv")) {
        train_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
        dest_train_file <- "pml-training.csv"
        download.file(train_data_url, dest_train_file)
}

if (!file.exists("pml_testing.csv")) {
        test_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
        dest_test_file <- "pml-testing.csv"
        download.file(test_data_url, dest_test_file)
}

## load them into memory
training <- read.csv("pml-training.csv", na.strings = c("NA", ""))
testing <- read.csv("pml-testing.csv", na.strings = c("NA", ""))

## lets look at the data to decide how to clean it
dim(testing); dim(training)
```
#### Cleaning data.
Splitting data into training and testing datasets. Removing NA's and first 7 lines of data, as they have no predictive value. 
```{r, cache=TRUE}
## load few libraries needed for further computations
library(caret); library(rattle); library(rpart); library(rpart.plot)
library(randomForest); library(repmis)

## make sure we create reproducible reaserch by setting the seed
set.seed(12345)

## split training data to prevent overfiting, 60% for training data and 40% for testing
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
my_training <- training[inTrain, ]
my_testing <- training[-inTrain, ]

## have a look with how big chunks we have to deal with
dim(my_training)
dim(my_testing)

## clean the data, remove columns where NAs occured (see how the model will perform, if the accuracy of final model is to low, the different approach will have to be put in place, like filling NAs with meens, etc)
my_training_2 <- my_training[, colSums(is.na(training)) == 0]

## remove first 7 columns as they have no predictive value
my_training_2 <- my_training_2[, -c(1:7)]
```
## Generating models.

### First model: classification tree. 
The first weapon of choise. It is fast, it is simple. Just about to find out how good it performs with given data.

Start with fitting the model. Next see how it has been built. Print out the conditions for splitting nodes in the model, and wrap it up in eyecandy plot with fancyRpartPlot().

The most important part of the first model, is to check it against saved test data selected from training data set. Printing summary of predicted values using confusionMatrix() will tell us some more about accuracy of the model, its error rate, will give us 95% confidence interval, p-value as well, as other usefull informations.
```{r, cache=TRUE}
## fit the first model: rpart
mod_fit_2 <- train(classe ~ ., method="rpart", data=my_training_2)

## lets have a look how the decision tree has been built
print(mod_fit_2$finalModel)
fancyRpartPlot(mod_fit_2$finalModel)

## predict testing values based on generated model and check accuracy
predictions_1 <- predict(mod_fit_2, newdata=my_testing)
confusionMatrix(predictions_1, my_testing$classe)
        ## very low accuracy, only 49%
```
#### Summary of the first model.
The accuracy for this model is very disapointing. with only 49.94% accuracy (50.06% error rate) it is actually better to flip a coin for guessing the outcome. 
It might be coused by badly chosen model fit. It could be a influence of the way we treated NAs in data set. Before we come back to the data, and start different approach to deal with missing values, lets have a look if other model will perform better.

### Second model: random forests.
The second approach is random forests. The model should perform better, then the previous one. If it wont, we will look for the couse of bad accuracy in the data cleaning process.

Here we will fit the model and use it against test data. Again, use the autput of confusionMatrix() to read error rate and accuracy of predictions.

I also found a way to visualize random forests models. As it is not as easy, as when plotting classification tree. The attempt is to show how the error rate drops down, depending on number of trees generated in the model.
```{r, cache=TRUE}
## fit the random forest model instead of decision trees

## mod_fit_3 <- train(classe ~ ., method="rf", data=my_training_2, prox=TRUE, ntree=5)
## takes for ages to compute, over 8h... :\ missed assigment because of this..
## trying different way

mod_fit_3 <- randomForest(classe ~ ., data=my_training_2)
## much faster then "rf" from caret package, although returns error on my machine when used
## with proximity=TRUE option. Not enought ram...

## do the prediction 
prediction_2 <- predict(mod_fit_3, newdata=my_testing)
confusionMatrix(prediction_2, my_testing$classe)
        ## excelent, 99.39% accuracy :)

## it is hard to plot random forest, the way i find usefull without doing proximity
## on the trees (which hangs my laptop dead) is this:
plot(mod_fit_3, log="y")
varImpPlot(mod_fit_3)
```

#### Summary of the second model.
This time the overall accuracy is 99.43%. This is an excellent result. Proves that classification tree wasn't a good approach to deal with this data. With error rate lower then 1% for all classes, it is clear, that with this model we can generate the results for testing data for course assigment.

## Prediction.

Using testing data set, we generate 20 files. Each one will have a name coresponding to specific test case, and contain A, B, C or D letter, sugesting in witch manner the excersize was performed.
```{r, cache=TRUE}
## predict answers for file submission
answers <- predict(mod_fit_3, newdata=testing)
answers

## generate files
pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}

pml_write_files(answers)
```

This completes the course project.
