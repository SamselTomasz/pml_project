---
title: "Practical Mashine Learning Project"
author: "Tomasz Samsel"
date: "10/25/2015"
output: html_document
---
split the r code into:
-load data
-clean data
-first approach
-second one
-generating txt files and writup 

## load data
```{r, cache=TRUE}
## download files if they do not exist 
if (!file.exists("pml_training.csv")) {
        train_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
        dest_train_file <- "pml-training.csv"
        download.file(train_data_url, dest_train_file)
}

if (!file.exists("pml_testing.csv")) {
        test_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
        dest_test_file <- "pml-testing.csv"
        download.file(test_data_url, dest_test_file)
}

## load them into memory
training <- read.csv("pml-training.csv", na.strings = c("NA", ""))
testing <- read.csv("pml-testing.csv", na.strings = c("NA", ""))

## lets look at the data to decide how to clean it
dim(testing); dim(training)
```

## clean data
```{r, cache=TRUE}
## load few libraries needed for further computations
library(caret); library(rattle); library(rpart); library(rpart.plot)
library(randomForest); library(repmis)

## make sure we create reproducible reaserch by setting the seed
set.seed(12345)

## split training data to prevent overfiting, 60% for training data and 40% for testing
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
my_training <- training[inTrain, ]
my_testing <- training[-inTrain, ]

## have a look with how big chunks we have to deal with
dim(my_training)
dim(my_testing)

## clean the data, remove columns where NAs occured (see how the model will perform, if the accuracy of final model is to low, the different approach will have to be put in place, like filling NAs with meens, etc)
my_training_2 <- my_training[, colSums(is.na(training)) == 0]

## remove first 7 columns as they have no predictive value
my_training_2 <- my_training_2[, -c(1:7)]
```

## first model
```{r, cache=TRUE}
## fit the first model: rpart
mod_fit_2 <- train(classe ~ ., method="rpart", data=my_training_2)

## lets have a look how the decision tree has been built
print(mod_fit_2$finalModel)
fancyRpartPlot(mod_fit_2$finalModel)

## predict testing values based on generated model and check accuracy
predictions_1 <- predict(mod_fit_2, newdata=my_testing)
confusionMatrix(predictions_1, my_testing$classe)
        ## very low accuracy, only 49%
```

## second model
```{r, cache=TRUE}
## fit the random forest model instead of decision trees

## mod_fit_3 <- train(classe ~ ., method="rf", data=my_training_2, prox=TRUE, ntree=5)
## takes for ages to compute, over 8h... :\ missed assigment because of this..
## trying different way

mod_fit_3 <- randomForest(classe ~ ., data=my_training_2)
## much faster then "rf" from caret package, although returns error on my machine when used
## with proximity=TRUE option. Not enought ram...

## do the prediction 
prediction_2 <- predict(mod_fit_3, newdata=my_testing)
confusionMatrix(prediction_2, my_testing$classe)
        ## excelent, 99.39% accuracy :)
```

## generating txt files over testing data provided
```{r, cache=TRUE}
## predict answers for file submission
answers <- predict(mod_fit_3, newdata=testing)
answers

## generate files
pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}

pml_write_files(answers)
```

## done :)
